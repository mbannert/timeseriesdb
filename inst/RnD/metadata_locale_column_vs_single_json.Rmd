---
title: "R Notebook"
output: html_notebook
---

```{r setup}
library(RPostgres)
library(data.table)
library(jsonlite)
library(microbenchmark)

con <- dbConnect(Postgres(), "postgres", "localhost", 1111, "pgpass", "postgres")

dbExecute(con, "
          CREATE TABLE IF NOT EXISTS meta_locale_col (
          id INTEGER,
          locale TEXT,
          data JSONB,
          PRIMARY KEY (id, locale)
          );")

dbExecute(con, "
          CREATE TABLE IF NOT EXISTS meta_json (
          id INTEGER PRIMARY KEY,
          data JSONB
          );")

n_records <- 100000;
locales <- c("de", "en", "fr", "it");
an_entry <- toJSON(list(
  field1 = "some such",
  field2 = "Cthulhu f'tagn!" # Never let a nerd do your coding. Then again... please do!
),
auto_unbox = TRUE)
```

We store `r n_records` meta data sets in the locales `r paste(locales, collapse = ", ")`.

```{r data}
records <- data.table(
  id = rep(seq(n_records), each = length(locales)),
  locale = locales,
  data = rep(an_entry, length(locales)*n_records)
)

dbExecute(con, "DELETE FROM meta_locale_col")
t0 <- Sys.time()
dbExecute(con, "
            INSERT INTO meta_locale_col VALUES ($1, $2, $3)
            ",
            list(
              records$id,
              records$locale,
              records$data
            )
          )
t_store_locale_col <- difftime(Sys.time(), t0, units = "secs")

single_record <- as.list(rep(an_entry, length(locales)))
names(single_record) <- locales
single_json <- toJSON(single_record, auto_unbox = TRUE)

dbExecute(con, "DELETE FROM meta_json")
t0 <- Sys.time()
dbExecute(con, "
          INSERT INTO meta_json VALUES ($1, $2)
          ",
          list(
            seq(n_records),
            rep(single_json, n_records)
          )
          )
t_store_json <- difftime(Sys.time(), t0, units = "secs")
```
Storing with a locale column took `r t_store_locale_col` seconds. Storing all in a single object took `r t_store_json`.
With a locale column we have `r length(locales)` records per id of course.


We shall now attempt to read an increasing number of records and see what happens.

```{r read}
cases <- 10^(1:4)
cases <- c(cases, 2*cases[length(cases)])

results <- list()
for(case in cases) {
  message(sprintf("now at %d", case))
  the_ids <- sample(n_records, size = case, replace = FALSE)
  
  results[[as.character(case)]] <- microbenchmark(
    dbGetQuery(con, "SELECT data FROM meta_locale_col WHERE locale = 'de' AND id IN ($1)", list(the_ids)),
    dbGetQuery(con, "SELECT data->'de' FROM meta_json WHERE id IN ($1)", list(the_ids)),
    times = 5
  )
}

times <- rbindlist(lapply(results, function(x) {
  m <- summary(x, unit = "ms")[, "mean"]
  list(
    locale_column = m[1],
    json = m[2]
  )
}))[, n_read := cases][, .(n_read, locale_column, json, locale_vs_json = 100*(locale_column/json))]

knitr::kable(times)
```

Smells OK to me really.

```{r teardown}
dbDisconnect(con)
```
